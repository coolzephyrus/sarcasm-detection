{"cells":[{"cell_type":"markdown","source":["# Part 2: Baseline Model using Pre-Trained Language Model\n","\n","This python notebook corresponds directly to the section 4.4 in the final thesis report. "],"metadata":{"id":"6L3xs8Rqa5mx"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"TL-uXqyZJqu9"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"MMlvAxgppNKk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Required Libraries"],"metadata":{"id":"CyySRl4-JjeE"}},{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"eSq_YnF5_f0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgetnug7jMBT"},"outputs":[],"source":["import os\n","import sys\n","import json\n","import random\n","import time\n","import re\n","import datetime\n","import pickle\n","\n","from tqdm import tqdm\n","\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n","\n","\n","from transformers import RobertaForSequenceClassification, BertForSequenceClassification,  DistilBertForSequenceClassification \n","from transformers import BertTokenizer, DistilBertTokenizer, RobertaTokenizer\n","from transformers import BertConfig, DistilBertConfig, RobertaConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup \n","\n","from transformers import logging\n","logging.set_verbosity_error()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"D4Su7DnC3o_Z"},"source":["### Model Configs"]},{"cell_type":"code","source":["# set path variables\n","basepath = '/content/gdrive/MyDrive/ljmu-ms-thesis/'\n","datapath = '/content/gdrive/MyDrive/ljmu-ms-thesis/data/'\n","modelpath =  '/content/gdrive/MyDrive/ljmu-ms-thesis/model/'"],"metadata":{"id":"R0lQU0r9QrbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common config settings\n","dataset = 'amazon_data.json'\n","dataset_path = os.path.join(datapath, dataset)\n","\n","EPOCHS = 10\n","TOTAL_ITERATIONS = 3\n","TEST_SIZE = 0.2\n","MAX_LEN = 32\n","BATCH_SIZE = 16\n","RANDOM_STATE = 2022\n","LEARNING_RATE = 2e-5\n","EPS = 1e-8\n","SEED_VAL = 42"],"metadata":{"id":"YuVVWgsBDs_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choose one model at a time\n","#model_name = 'bert'\n","#model_name = 'roberta'\n","model_name = 'distilbert'"],"metadata":{"id":"_49WkYN6IWD6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"6t-RQf2H8bPQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF1yFZgP3o_b"},"outputs":[],"source":["def load_dataset(filename):\n","    dataset = []\n","    with open(filename) as f:\n","        for line in f:\n","            entry = {}\n","            \n","            line = line.strip()\n","            d = json.loads(line)\n","            \n","            entry['sentence'] = d['sentence']\n","            entry['label'] = int(d['label'])\n","            dataset.append(entry)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9m95-SAW3o_d"},"outputs":[],"source":["dataset = load_dataset(dataset_path)"]},{"cell_type":"markdown","metadata":{"id":"pbKZGfiI3o_d"},"source":["### Data Pre-processing"]},{"cell_type":"code","source":["# Initialise tokenizer\n","def get_tokenizer(model_name):\n","    if model_name == 'distilbert':\n","        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","    elif model_name == 'bert':\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","    elif model_name == 'roberta':\n","        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","    return tokenizer\n","\n","tokenizer = get_tokenizer(model_name)"],"metadata":{"id":"JGRT9x9EH5T6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3ikyMoV3o_e"},"outputs":[],"source":["# Tokenization and sentence embedding\n","def encode_all(sentences, model_name):  \n","    input_ids = []\n","    for data in sentences:\n","        input_ids.append(tokenizer.encode(data, add_special_tokens=True))\n","    return input_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4bWkCnE3o_f"},"outputs":[],"source":["# adding attention mask\n","def get_attn(input_ids):\n","    attention_masks = []\n","    for sent in input_ids:\n","        att_mask = [int(token_id > 0) for token_id in sent]\n","        attention_masks.append(att_mask)\n","    return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjRhA8Ep3o_f"},"outputs":[],"source":["# Create Dataset\n","def create_dataset(dataset):\n","    all_data = []\n","    for data in tqdm(dataset):\n","        input_ids = []\n","        input_ids.append(tokenizer.encode(data['sentence'].lower()))\n","        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')\n","        input_ids = torch.tensor(input_ids)\n","        \n","        attn_mask = torch.tensor(get_attn(input_ids))\n","\n","        entry = {}\n","        entry['raw_sentence'] = data['sentence']\n","        entry['sentence'] = input_ids[0]\n","        entry['sentence_mask'] = attn_mask[0]\n","        \n","        all_data.append((entry, data['label']))\n","    return all_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rydF0dM13o_g"},"outputs":[],"source":["dataset_list = create_dataset(dataset)"]},{"cell_type":"markdown","metadata":{"id":"swdgjID63o_h"},"source":["### Training & Validation Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIiUUJkl3o_h"},"outputs":[],"source":["trainset, validationset = train_test_split(dataset_list, random_state=RANDOM_STATE, test_size=TEST_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJhKcShz3o_h"},"outputs":[],"source":["def save_dataset():\n","    with open(modelpath + 'trainset-' + model_name +'.data', 'wb') as f:\n","        pickle.dump(trainset, f)\n","\n","    with open(modelpath + 'validationset-' + model_name + '.data', 'wb') as f:\n","        pickle.dump(validationset, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zbrUrOS3o_h"},"outputs":[],"source":["save_dataset()"]},{"cell_type":"markdown","metadata":{"id":"_5XDjKNz3o_i"},"source":["### Data Loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq8XyD4R3o_i"},"outputs":[],"source":["train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(validationset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIWvHG0k3o_i"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"5b8JR3QJ3o_i"},"source":["## Model Initialisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZshzSQD3o_i"},"outputs":[],"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","\n","def reset_model(model_name):\n","    if model_name == 'distilbert':\n","        model = DistilBertForSequenceClassification.from_pretrained(\n","                                                'distilbert-base-uncased',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = True, \n","                                            )\n","    elif model_name == 'roberta':\n","        model = RobertaForSequenceClassification.from_pretrained(\n","                                                'roberta-base',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = True, \n","                                            )\n","    elif model_name == 'bert':\n","        model = BertForSequenceClassification.from_pretrained(\n","                                                'bert-base-uncased',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = True, \n","                                            )\n","    model.to(device)\n","    \n","    optimizer = AdamW(model.parameters(),\n","                  lr = LEARNING_RATE,\n","                  eps = EPS \n","                )\n","\n","    total_steps = len(train_loader) * EPOCHS\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps)\n","    return model, optimizer, scheduler"]},{"cell_type":"markdown","metadata":{"id":"g-mHsKCX3o_j"},"source":["### Utility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"789PLttV3o_j"},"outputs":[],"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"markdown","metadata":{"id":"aF5QwJnH3o_j"},"source":["### Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zidJRjk3o_j"},"outputs":[],"source":["random.seed(SEED_VAL)\n","np.random.seed(SEED_VAL)\n","torch.manual_seed(SEED_VAL)\n","torch.cuda.manual_seed_all(SEED_VAL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOE73Gh73o_j"},"outputs":[],"source":["def train(model, optimizer, scheduler):\n","    total_loss = 0\n","    model.train()\n","    for step, (batch, labels) in enumerate(train_loader):\n","        b_input_ids = batch['sentence'].to(device)\n","        b_input_mask = batch['sentence_mask'].to(device)\n","        b_labels = labels.to(device)\n","        model.zero_grad()        \n","        \n","        outputs = model(b_input_ids, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","                \n","        loss = outputs[0]\n","\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_loader)"]},{"cell_type":"markdown","source":["### Testing the Model"],"metadata":{"id":"Ji2awTDX8tSB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJimA00C3o_k"},"outputs":[],"source":["def test(model):\n","    \n","    t0 = time.time()\n","    model.eval()\n","    \n","    all_predictions = []\n","    all_labels = []\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    for batch, b_labels in test_loader:\n","        \n","        with torch.no_grad():        \n","            outputs = model(batch['sentence'].to(device), \n","                            attention_mask=batch['sentence_mask'].to(device))\n","        \n","        logits = outputs[0]\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        prediction = list(np.argmax(logits, axis=1).flatten())\n","        all_predictions.extend(prediction)\n","        all_labels.extend(label_ids.flatten())\n","        \n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    \n","    matrix = confusion_matrix(all_predictions, all_labels)\n","\n","    tp = matrix[0][0]\n","    fp = matrix[0][1]\n","    fn = matrix[1][0]\n","    tn = matrix[1][1]\n","\n","    accuracy = eval_accuracy/nb_eval_steps\n","    f1 = f1_score(all_predictions, all_labels, average = 'macro')\n","    precision = precision_score(all_predictions, all_labels, average = 'macro')\n","    recall = recall_score(all_predictions, all_labels, average = 'macro')\n","    \n","    return [f1, precision, recall, accuracy]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fPL_E3t3o_k"},"outputs":[],"source":["def save_model(filename = modelpath + model_name +'_baseline.pb'):\n","    torch.save(model, filename)"]},{"cell_type":"markdown","source":["### Run the model"],"metadata":{"id":"tbwtomah8xJ_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HIfjicQ3o_k"},"outputs":[],"source":["# Run the model\n","def run(model, optimizer, scheduler):\n","    loss_values = []\n","    best_results = [0, 0, 0, 0]\n","    t0 = time.time()\n","    \n","    for epoch_i in range(0, EPOCHS):\n","        train(model, optimizer, scheduler)\n","        elapsed = format_time(time.time() - t0)\n","        print('Epoch {:} / {:}    Elapsed: {:}.'.format(epoch_i + 1, EPOCHS, elapsed))\n","        results = test(model)\n","        if best_results[0] < results[0]:\n","            best_results = results\n","            save_model()\n"," \n","    return best_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzCxsgur3o_k"},"outputs":[],"source":["# Display the results\n","all_results = []\n","t0 = time.time()\n","\n","for iteration in range(TOTAL_ITERATIONS):\n","    it0 = time.time()\n","    print('-'*50)\n","    print('Iteration {:2d}'.format(iteration+1))\n","    print('-'*50)\n","    model, optimizer, scheduler = reset_model(model_name)\n","    result = run(model, optimizer, scheduler)\n","    all_results.append(result)\n","    print('-'*50)\n","    print('Result for this iteration: ', result)\n","    print('Time taken for this iteration: {:}'.format(format_time(time.time() - it0)))\n","\n","# Final results is the average of all the iterations\n","final_results = [sum(value)/len(value) for value in zip(*all_results)]\n","\n","print('-'*50)\n","print('Final Results for the Baseline ' + upper(model_name) + ' model')\n","print('-'*50)\n","print('F1-score: {0:.4f}'.format(final_results[0]))\n","print('Precision: {0:.4f}'.format(final_results[1]))\n","print('Recall: {0:.4f}'.format(final_results[2]))\n","print('Accuracy: {0:.4f}'.format(final_results[3]))\n","print('Time Taken: {:}'.format(format_time(time.time() - t0)))\n","print('-'*50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6dCIYtJ3o_l"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"name":"baseline-plm.ipynb","provenance":[{"file_id":"1PmJH1DXF28uQyy3PCajjSYfCnhPuzW0w","timestamp":1654252637251},{"file_id":"1uWmHCxoflZVEminogHYZ2G3F_JckZpay","timestamp":1653731754623}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}