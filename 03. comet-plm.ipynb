{"cells":[{"cell_type":"markdown","source":["# Part 2: Baseline Model using Pre-Trained Language Model\n","\n","This python notebook corresponds directly to the section 4.4 in the final thesis report. "],"metadata":{"id":"6L3xs8Rqa5mx"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"TL-uXqyZJqu9"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"MMlvAxgppNKk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Required Libraries"],"metadata":{"id":"CyySRl4-JjeE"}},{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"eSq_YnF5_f0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install git+https://github.com/vered1986/comet-commonsense.git"],"metadata":{"id":"5qEGjJhnjN2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgetnug7jMBT"},"outputs":[],"source":["import os\n","import sys\n","import json\n","import random\n","import time\n","import re\n","import datetime\n","import pickle\n","\n","from tqdm import tqdm\n","\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n","\n","from comet2.comet_model import PretrainedCometModel\n","\n","from transformers import BertForSequenceClassification,  DistilBertForSequenceClassification, RobertaForSequenceClassification\n","from transformers import BertForMaskedLM, DistilBertForMaskedLM, RobertaForMaskedLM\n","from transformers import BertTokenizer, DistilBertTokenizer, RobertaTokenizer\n","\n","from transformers import BertConfig, DistilBertConfig, RobertaConfig\n","from transformers import AdamW, get_linear_schedule_with_warmup \n","\n","from transformers import logging\n","logging.set_verbosity_error()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"D4Su7DnC3o_Z"},"source":["### Model Configs"]},{"cell_type":"code","source":["# set path variables\n","basepath = '/content/gdrive/MyDrive/ljmu-ms-thesis/'\n","datapath = '/content/gdrive/MyDrive/ljmu-ms-thesis/data/'\n","modelpath =  '/content/gdrive/MyDrive/ljmu-ms-thesis/model/'"],"metadata":{"id":"R0lQU0r9QrbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common config settings\n","EPOCHS = 10\n","TOTAL_ITERATIONS = 3\n","TEST_SIZE = 0.2\n","MAX_LEN = 32\n","BATCH_SIZE = 16\n","RANDOM_STATE = 2022\n","LEARNING_RATE = 2e-5\n","EPS = 1e-8\n","SEED_VAL = 42"],"metadata":{"id":"YuVVWgsBDs_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choose one model at a time\n","#model_name = 'bert'\n","#model_name = 'roberta'\n","model_name = 'distilbert'"],"metadata":{"id":"_49WkYN6IWD6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbKZGfiI3o_d"},"source":["### Common Sense Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0K2-FgJjMBb"},"outputs":[],"source":["comet_model = PretrainedCometModel(device=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6vmeQhzjMBc"},"outputs":[],"source":["CATEGORIES = [\"oReact\", \"oEffect\", \"oWant\", \"xAttr\", \"xEffect\", \"xIntent\", \"xNeed\", \"xReact\", \"xWant\"]\n","\n","def gather_commonsense(sentence):\n","    commonsense = {}\n","    for c in CATEGORIES:\n","        commonsense[c] = comet_model.predict(sentence, c, num_beams = 8)\n","    return commonsense\n","\n","def get_comet_data(filename):\n","    num_lines = sum(1 for line in open(filename,'r'))\n","    content = []\n","    with open(filename) as f:\n","      for line in tqdm(f, total = num_lines):\n","        linedata = json.loads(line.strip())\n","        entry = {}\n","        entry['sentence'] = linedata['sentence'].lower()\n","        entry['label'] = linedata['label']        \n","        entry['common_sense'] = gather_commonsense(entry['sentence'])\n","        content.append(json.dumps(entry))        \n","    return  content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VJoYmdxjMBd"},"outputs":[],"source":["def write_content(filename, content):\n","    with open(filename, \"w\") as f:\n","        for c in content:\n","            f.write(c + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyxpYHHXjMBd"},"outputs":[],"source":["def extract_commonsense():\n","    read_filename = 'amazon_data.json'\n","    write_filename = 'amazon_'+ model_name +'_comet.json'\n","    content = get_comet_data(datapath + read_filename)\n","    write_content(datapath + write_filename, content)\n","    return content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69LzuXz7jMBd"},"outputs":[],"source":["# Extract commonsense. This is the long running part. Run this just once for each model\n","# content = extract_commonsense() "]},{"cell_type":"markdown","metadata":{"id":"Tk9XSB9sjMBe"},"source":["### Auto complete COMET sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1XuyUGY1mz1"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZNHkI721mz1"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased', output_attentions=True)\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrU5HHSz1mz2"},"outputs":[],"source":["def get_sentence(trail, subject):\n","    text = '[CLS] ' + subject + ' [MASK] ' + trail +'[SEP]'\n","    tokenized_text = tokenizer.tokenize(text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","    masked_index = tokenized_text.index('[MASK]')\n","    \n","    # Create the segments tensors.\n","    segments_ids = [0] * len(tokenized_text)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n","    segments_tensors = torch.tensor([segments_ids]).to(device)\n","\n","    # Predict all tokens\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","        predictions = outputs[0]\n","        attention = outputs[-1]\n","    predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","\n","    return subject + ' ' + predicted_token + ' ' + trail"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrlLYJN81mz2"},"outputs":[],"source":["def get_subject(text):\n","    doc = nlp(text)\n","    sentence = next(doc.sents) \n","    for w in sentence:\n","        if w.dep_ == 'nsubj':\n","            return w.text\n","    return \"\"\n","\n","def process_concepts(subject, concept_list):\n","    for c in concept_list:\n","        if c == 'none':\n","            continue\n","            \n","        if subject == '':\n","            return c\n","        \n","        return get_sentence(c, subject)\n","\n","def reprocess_dataset(filename):\n","    num_lines = sum(1 for line in open(filename,'r'))\n","    content = []\n","    \n","    with open(filename) as f:\n","        for line in tqdm(f, total = num_lines):\n","            new_entry = {}\n","            entry = json.loads(line.strip())\n","            new_entry['sentence'] = entry['sentence']\n","            new_entry['label'] = entry['label']\n","            new_entry['common_sense'] = {}\n","            \n","            s = get_subject(entry['sentence'])\n","            \n","            for c in entry['common_sense'].keys():\n","                if c in ['xWant', 'xNeed', 'xIntent', 'xEffect']:\n","                    new_entry['common_sense'][c] = process_concepts(s, entry['common_sense'][c])\n","            content.append(new_entry)\n","    return content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rmxcWqP1mz2"},"outputs":[],"source":["def write_to_file(filename, content):\n","    with open(filename, \"w\") as f:\n","        for line in content:\n","            f.write(json.dumps(line) + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aujJ89dQ1mz2"},"outputs":[],"source":["def autocomplete_dataset():\n","    read_filename = 'amazon_' + model_name + '_comet.json'\n","    write_filename = 'amazon_' + model_name + '_comet_autocomplete.json'\n","    content = reprocess_dataset(datapath + read_filename)\n","    write_to_file(datapath + write_filename, content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75ijRBUT1mz3"},"outputs":[],"source":["# Run the above function once for each LM\n","# autocomplete_dataset() "]},{"cell_type":"code","source":["def load_dataset(filename):\n","    dataset = []\n","    with open(filename) as f:\n","        for line in f:\n","            entry = {}\n","            \n","            line = line.strip()\n","            d = json.loads(line)\n","            \n","            entry['sentence'] = d['sentence']\n","            entry['label'] = int(d['label'])\n","            entry['support'] = []\n","\n","            for k in d['common_sense'].keys():\n","                if k == 'xWant' or k == 'xEffect':\n","                    entry['support'].append(d['common_sense'][k])\n","            dataset.append(entry)\n","    return dataset"],"metadata":{"id":"Krx5cJlWDWIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_filename = 'amazon_' + model_name + '_comet_autocomplete.json'\n","dataset = load_dataset(datapath + read_filename)"],"metadata":{"id":"wY434Z2xDag_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Pre-processing"],"metadata":{"id":"YKRtlRN4HhbI"}},{"cell_type":"code","source":["# Initialise tokenizer\n","def get_tokenizer(model_name):\n","    if model_name == 'distilbert':\n","        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","    elif model_name == 'bert':\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","    elif model_name == 'roberta':\n","        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","    return tokenizer\n","\n","tokenizer = get_tokenizer(model_name)"],"metadata":{"id":"a9QPZUxxHsv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3ikyMoV3o_e"},"outputs":[],"source":["# Tokenization and sentence embedding\n","def encode_all(sentences, model_name):  \n","    input_ids = []\n","    for data in sentences:\n","        input_ids.append(tokenizer.encode(data, add_special_tokens=True))\n","    return input_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4bWkCnE3o_f"},"outputs":[],"source":["# adding attention mask\n","def get_attn(input_ids):\n","    attention_masks = []\n","    for sent in input_ids:\n","        att_mask = [int(token_id > 0) for token_id in sent]\n","        attention_masks.append(att_mask)\n","    return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjRhA8Ep3o_f"},"outputs":[],"source":["# Create Dataset\n","def create_dataset(dataset):\n","    all_data = []\n","    for data in tqdm(dataset):\n","        input_ids = []\n","        input_ids.append(tokenizer.encode(data['sentence'].lower()))\n","\n","        for s in data['support']:\n","            input_ids.append(tokenizer.encode(s.lower()))\n","\n","        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')\n","        input_ids = torch.tensor(input_ids)\n","        \n","        attn_mask = torch.tensor(get_attn(input_ids))\n","\n","        entry = {}\n","        #entry['raw_sentence'] = data['sentence']\n","        entry['raw_sentence'] = data['sentence'] + \" [SUPPORT]: \" + \" [SEP] \".join(data['support'])\n","        entry['sentence'] = input_ids[0]\n","        entry['sentence_mask'] = attn_mask[0]\n","        \n","        entry['support'] = input_ids[1:]\n","        entry['support_mask'] = attn_mask[1:]\n","\n","        all_data.append((entry, data['label']))\n","    return all_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rydF0dM13o_g"},"outputs":[],"source":["dataset_list = create_dataset(dataset)"]},{"cell_type":"markdown","metadata":{"id":"swdgjID63o_h"},"source":["### Training & Validation Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIiUUJkl3o_h"},"outputs":[],"source":["trainset, validationset = train_test_split(dataset_list, random_state=RANDOM_STATE, test_size=TEST_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJhKcShz3o_h"},"outputs":[],"source":["def save_dataset():\n","    with open(modelpath + 'trainset-comet-' + model_name +'.data', 'wb') as f:\n","        pickle.dump(trainset, f)\n","\n","    with open(modelpath + 'validationset-comet-' + model_name + '.data', 'wb') as f:\n","        pickle.dump(validationset, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zbrUrOS3o_h"},"outputs":[],"source":["save_dataset()"]},{"cell_type":"markdown","metadata":{"id":"_5XDjKNz3o_i"},"source":["### Data Loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq8XyD4R3o_i"},"outputs":[],"source":["train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(validationset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIWvHG0k3o_i"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"5b8JR3QJ3o_i"},"source":["### Model Initialisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZshzSQD3o_i"},"outputs":[],"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","\n","def reset_model(model_name):\n","    if model_name == 'distilbert':\n","        model = DistilBertForSequenceClassification.from_pretrained(\n","                                                'distilbert-base-uncased',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = False, \n","                                            )\n","    elif model_name == 'roberta':\n","        model = RobertaForSequenceClassification.from_pretrained(\n","                                                'roberta-base',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = True, \n","                                            )\n","    elif model_name == 'bert':\n","        model = BertForSequenceClassification.from_pretrained(\n","                                                'bert-base-uncased',\n","                                                num_labels = 2, \n","                                                output_attentions = True, \n","                                                output_hidden_states = True, \n","                                            )\n","    model.to(device)\n","    \n","    optimizer = AdamW(model.parameters(),\n","                  lr = LEARNING_RATE,\n","                  eps = EPS \n","                )\n","\n","    total_steps = len(train_loader) * EPOCHS\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps)\n","    return model, optimizer, scheduler"]},{"cell_type":"markdown","metadata":{"id":"g-mHsKCX3o_j"},"source":["### Utility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"789PLttV3o_j"},"outputs":[],"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"markdown","metadata":{"id":"aF5QwJnH3o_j"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zidJRjk3o_j"},"outputs":[],"source":["random.seed(SEED_VAL)\n","np.random.seed(SEED_VAL)\n","torch.manual_seed(SEED_VAL)\n","torch.cuda.manual_seed_all(SEED_VAL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOE73Gh73o_j"},"outputs":[],"source":["def train(model, optimizer, scheduler):\n","    total_loss = 0\n","    model.train()\n","    for step, (batch, labels) in enumerate(train_loader):\n","        b_input_ids = batch['sentence'].to(device)\n","        b_input_mask = batch['sentence_mask'].to(device)\n","        b_labels = labels.to(device)\n","        model.zero_grad()        \n","        \n","        outputs = model(b_input_ids, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","                \n","        loss = outputs[0]\n","\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJimA00C3o_k"},"outputs":[],"source":["def test(model):\n","    \n","    t0 = time.time()\n","    model.eval()\n","    \n","    all_predictions = []\n","    all_labels = []\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    for batch, b_labels in test_loader:\n","        \n","        with torch.no_grad():        \n","            outputs = model(batch['sentence'].to(device), \n","                            attention_mask=batch['sentence_mask'].to(device))\n","        \n","        logits = outputs[0]\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        prediction = list(np.argmax(logits, axis=1).flatten())\n","        all_predictions.extend(prediction)\n","        all_labels.extend(label_ids.flatten())\n","        \n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    \n","    matrix = confusion_matrix(all_predictions, all_labels)\n","\n","    tp = matrix[0][0]\n","    fp = matrix[0][1]\n","    fn = matrix[1][0]\n","    tn = matrix[1][1]\n","\n","    accuracy = eval_accuracy/nb_eval_steps\n","    f1 = f1_score(all_predictions, all_labels, average = 'macro')\n","    precision = precision_score(all_predictions, all_labels, average = 'macro')\n","    recall = recall_score(all_predictions, all_labels, average = 'macro')\n","\n","    return [f1, precision, recall, accuracy]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fPL_E3t3o_k"},"outputs":[],"source":["def save_model(filename = modelpath + model_name +'_comet.pb'):\n","    torch.save(model, filename)"]},{"cell_type":"markdown","source":["### Run the model"],"metadata":{"id":"tbwtomah8xJ_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HIfjicQ3o_k"},"outputs":[],"source":["def run(model, optimizer, scheduler):\n","    best_results = [0, 0, 0, 0]\n","    t0 = time.time()\n","    \n","    for epoch_i in range(0, EPOCHS):\n","        train(model, optimizer, scheduler)\n","        elapsed = format_time(time.time() - t0)\n","        print('Epoch {:} / {:}    Elapsed: {:}.'.format(epoch_i + 1, EPOCHS, elapsed))\n","        results = test(model)\n","        if best_results[0] < results[0]:\n","            best_results = results\n","            save_model()\n"," \n","    return best_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzCxsgur3o_k"},"outputs":[],"source":["all_results = []\n","t0 = time.time()\n","\n","for iteration in range(TOTAL_ITERATIONS):\n","    it0 = time.time()\n","    print('-'*50)\n","    print('Iteration {:2d}'.format(iteration+1))\n","    print('-'*50)\n","    model, optimizer, scheduler = reset_model(model_name)\n","    result = run(model, optimizer, scheduler)\n","    all_results.append(result)\n","    print('-'*50)\n","    print('Result for this iteration: ', result)\n","    print('Time taken for this iteration: {:}'.format(format_time(time.time() - it0)))\n","\n","# Final results is the average of all the iterations\n","final_results = [sum(value)/len(value) for value in zip(*all_results)]\n","\n","print('-'*50)\n","print('Final Results for the COMET + ' + model_name.upper() + ' model')\n","print('-'*50)\n","print('F1-score: {0:.4f}'.format(final_results[0]))\n","print('Precision: {0:.4f}'.format(final_results[1]))\n","print('Recall: {0:.4f}'.format(final_results[2]))\n","print('Accuracy: {0:.4f}'.format(final_results[3]))\n","print('Time Taken: {:}'.format(format_time(time.time() - t0)))\n","print('-'*50)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"name":"comet-plm.ipynb","provenance":[{"file_id":"1PmJH1DXF28uQyy3PCajjSYfCnhPuzW0w","timestamp":1654252637251},{"file_id":"1uWmHCxoflZVEminogHYZ2G3F_JckZpay","timestamp":1653731754623}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}